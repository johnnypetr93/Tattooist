{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f683cde",
   "metadata": {},
   "source": [
    "<h1>Tattooist</h1> <br>\n",
    "Tattooist as an overall product is a reservation system for tattoo artists. This part of the project focuses on one of the key features - recommendation system. \n",
    "\n",
    "Tattoo artists upload examples of their work. These photos are then used as training data for image classification model which serves as a recommendation system for customers. A user uploads a picture of their desired tattoo style from the Internet. Based on this photo, said classification model recommends suitable tattoo artists in the area.\n",
    "\n",
    "<i>Still in progress. Not publicly available in production yet.</i>\n",
    "\n",
    "The current workflow is that the classification model is converted to a CoreML model which is meant to sit in cloud and be downloaded each time a user opens an app (providing a new version of the model is available). It is to be yet discussed whether sending API requests to a model in cloud would not be the more efficient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8429855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scikit-learn version 1.3.0 is not supported. Minimum required version: 0.17. Maximum required version: 1.1.2. Disabling scikit-learn conversion API.\n",
      "TensorFlow version 2.15.0 has not been tested with coremltools. You may run into unexpected errors. TensorFlow 2.12.0 is the most recent version that has been tested.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import coremltools as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9a9eead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 478 images belonging to 10 classes.\n",
      "Found 48 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    rescale=1./255) \n",
    "\n",
    "# Photos directory (currently local, in full production in AWS S3)\n",
    "train_dir = 'DataV3/Train'\n",
    "validation_dir = 'DataV3/Valid'\n",
    "\n",
    "# Load images from the directories\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='sparse')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2c049bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Directory:\n",
      "Number of photos in dariastahp: 36\n",
      "Number of photos in bod.yx: 30\n",
      "Number of photos in maison_hefner: 29\n",
      "Number of photos in rotopet: 54\n",
      "Number of photos in gabi.tetu: 63\n",
      "Number of photos in sunshine_ink: 65\n",
      "Number of photos in obrazkynatelo: 31\n",
      "Number of photos in bronislava_orlicka: 37\n",
      "Number of photos in daf647: 73\n",
      "Number of photos in duhovka.ink: 70\n",
      "\n",
      "Validation Directory:\n",
      "Number of photos in dariastahp: 5\n",
      "Number of photos in bod.yx: 5\n",
      "Number of photos in maison_hefner: 3\n",
      "Number of photos in rotopet: 5\n",
      "Number of photos in gabi.tetu: 5\n",
      "Number of photos in sunshine_ink: 5\n",
      "Number of photos in obrazkynatelo: 5\n",
      "Number of photos in bronislava_orlicka: 5\n",
      "Number of photos in daf647: 5\n",
      "Number of photos in duhovka.ink: 5\n"
     ]
    }
   ],
   "source": [
    "def count_photos_in_subfolders(directory):\n",
    "    for subdir, dirs, files in os.walk(directory):\n",
    "        if subdir == directory:\n",
    "            continue\n",
    "        photo_count = len(files)\n",
    "        subfolder_name = os.path.basename(subdir)\n",
    "        print(f\"Number of photos in {subfolder_name}: {photo_count}\")\n",
    "\n",
    "print(\"Train Directory:\")\n",
    "count_photos_in_subfolders('DataV3/Train')\n",
    "\n",
    "print(\"\\nValidation Directory:\")\n",
    "count_photos_in_subfolders('DataV3/Valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20c1719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_validation_samples = len(validation_generator.filenames)\n",
    "batch_size = validation_generator.batch_size\n",
    "validation_steps = total_validation_samples // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0c9f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(224, 224, 3))\n",
    "base_model = DenseNet121(include_top=False, input_shape=(224, 224, 3), weights='imagenet', input_tensor=input_tensor)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(units=10, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d79f7630",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_training_images = 478\n",
    "batch_size = 32\n",
    "steps_per_epoch = total_training_images / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d225b732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "14/14 [==============================] - 21s 1s/step - loss: 2.9943 - accuracy: 0.1360 - val_loss: 2.5647 - val_accuracy: 0.1250\n",
      "Epoch 2/15\n",
      "14/14 [==============================] - 15s 953ms/step - loss: 2.5642 - accuracy: 0.1778 - val_loss: 2.2472 - val_accuracy: 0.2188\n",
      "Epoch 3/15\n",
      "14/14 [==============================] - 14s 907ms/step - loss: 2.2681 - accuracy: 0.2469 - val_loss: 1.8766 - val_accuracy: 0.3125\n",
      "Epoch 4/15\n",
      "14/14 [==============================] - 14s 931ms/step - loss: 2.1268 - accuracy: 0.3285 - val_loss: 1.7243 - val_accuracy: 0.4375\n",
      "Epoch 5/15\n",
      "14/14 [==============================] - 14s 935ms/step - loss: 1.8963 - accuracy: 0.3577 - val_loss: 1.5681 - val_accuracy: 0.5000\n",
      "Epoch 6/15\n",
      "14/14 [==============================] - 14s 933ms/step - loss: 1.6570 - accuracy: 0.4582 - val_loss: 1.3111 - val_accuracy: 0.6875\n",
      "Epoch 7/15\n",
      "14/14 [==============================] - 14s 917ms/step - loss: 1.5811 - accuracy: 0.4791 - val_loss: 1.3078 - val_accuracy: 0.6875\n",
      "Epoch 8/15\n",
      "14/14 [==============================] - 14s 935ms/step - loss: 1.5348 - accuracy: 0.4854 - val_loss: 1.3802 - val_accuracy: 0.5938\n",
      "Epoch 9/15\n",
      "14/14 [==============================] - 14s 936ms/step - loss: 1.4768 - accuracy: 0.5021 - val_loss: 1.2653 - val_accuracy: 0.6875\n",
      "Epoch 10/15\n",
      "14/14 [==============================] - 14s 942ms/step - loss: 1.3968 - accuracy: 0.5418 - val_loss: 1.2598 - val_accuracy: 0.5938\n",
      "Epoch 11/15\n",
      "14/14 [==============================] - 14s 941ms/step - loss: 1.3025 - accuracy: 0.5669 - val_loss: 1.1270 - val_accuracy: 0.6250\n",
      "Epoch 12/15\n",
      "14/14 [==============================] - 14s 950ms/step - loss: 1.2588 - accuracy: 0.6025 - val_loss: 1.0032 - val_accuracy: 0.7188\n",
      "Epoch 13/15\n",
      "14/14 [==============================] - 14s 929ms/step - loss: 1.1497 - accuracy: 0.6130 - val_loss: 0.9429 - val_accuracy: 0.7188\n",
      "Epoch 14/15\n",
      "14/14 [==============================] - 15s 944ms/step - loss: 1.1008 - accuracy: 0.6548 - val_loss: 0.8840 - val_accuracy: 0.7188\n",
      "Epoch 15/15\n",
      "14/14 [==============================] - 15s 948ms/step - loss: 1.0595 - accuracy: 0.6485 - val_loss: 0.9381 - val_accuracy: 0.6875\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch, \n",
    "    epochs=15,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c6cea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Keras model\n",
    "\n",
    "model.save('Keras/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35d8c353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running TensorFlow Graph Passes: 100%|███████| 6/6 [00:00<00:00, 17.76 passes/s]\n",
      "Converting TF Frontend ==> MIL Ops: 100%|█| 1099/1099 [00:00<00:00, 4801.35 ops/\n",
      "Running MIL frontend_tensorflow2 pipeline: 100%|█| 7/7 [00:00<00:00, 344.98 pass\n",
      "Running MIL default pipeline: 100%|████████| 69/69 [00:01<00:00, 45.81 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|█| 9/9 [00:00<00:00, 477.90 pas\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|█| 1611/1611 [00:02<00:00, 670.51 op\n"
     ]
    }
   ],
   "source": [
    "# Getting class names for the CoreML model\n",
    "\n",
    "class_indices = train_generator.class_indices\n",
    "class_names = sorted(class_indices, key=class_indices.get)  # Sort class names by their indices\n",
    "print(class_names)\n",
    "\n",
    "# Converting model to CoreML model suitable for iOS applications\n",
    "\n",
    "mlmodel = ct.convert(model, inputs=[ct.ImageType(scale=1/255.0)], \n",
    "                    classifier_config=ct.ClassifierConfig(class_names),\n",
    "                     convert_to=\"neuralnetwork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78cf9870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving CoreML model\n",
    "\n",
    "ct.models.utils.save_spec(mlmodel.get_spec(), 'CoreML.mlpackage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bc894a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
